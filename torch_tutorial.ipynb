{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Tutorial",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik9M2igT0XWf"
      },
      "source": [
        "import collections, time, math, random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWus4HvyKtos"
      },
      "source": [
        "# Writing a simple model in PyTorch\n",
        "\n",
        "This notebook shows you how to get started with PyTorch and also provides you some skeleton code. You can make a copy of the notebook and write your solution in it, or you can download it (**File &rarr; Download .py**) and work on it locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S13j4P_ZLjK5"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Clone the HW1 repository. (If you rerun the notebook, you'll get an error that directory `hw1` already exists, which you can ignore.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICSyqjzMLk2C",
        "outputId": "e00e95ef-f552-41fc-e54d-2806fe28fb45"
      },
      "source": [
        "!git clone https://github.com/ND-CSE-40657/hw1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw1'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 79 (delta 16), reused 36 (delta 10), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (79/79), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-6Ff7j9Qu_v"
      },
      "source": [
        "Import PyTorch. If you want to run on your own computer, you'll need to install PyTorch, which is usually as simple as `pip install torch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1gqmSDrQ6Nc",
        "outputId": "72f6b060-73f8-4b29-b947-b4e9e040297d"
      },
      "source": [
        "import torch\n",
        "print(f'Using Torch v{torch.__version__}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Torch v1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bNQMVSzTQu0"
      },
      "source": [
        "Check for a GPU. A GPU is not necessary for this assignment -- in fact, for the size of model we're training, it probably makes things slower. To enable/disable GPU, go to **Runtime &rarr; Change runtime type &rarr; Hardware accelerator** and select **GPU** (to enable the GPU) or **None** (to disable the GPU)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhOxkWlJQu5Q",
        "outputId": "bc3e0fa1-d2ed-4178-dcd6-64c65f1fdaf2"
      },
      "source": [
        "if torch.cuda.device_count() > 0:\n",
        "    print(f'Using GPU ({torch.cuda.get_device_name(0)})')\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print('Using CPU')\n",
        "    device = 'cpu'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s02IiCdSbno"
      },
      "source": [
        "## Read and preprocess data\n",
        "\n",
        "Read in the data files. Note that we strip trailing newlines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJipdajNwBZ-"
      },
      "source": [
        "def read_data(filename):\n",
        "    return [list(line.rstrip('\\n')) + ['<EOS>'] for line in open(filename)]\n",
        "traindata = read_data('hw1/data/train')\n",
        "devdata = read_data('hw1/data/dev')\n",
        "testdata = read_data('hw1/data/test')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiioHKwZxFqe"
      },
      "source": [
        "Create a vocabulary containing the most frequent words and some special words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmt1wc9zwTNN"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, counts, size):\n",
        "        self.size = size\n",
        "        words = {'<EOS>', '<UNK>'}\n",
        "        for word, _ in counts.most_common():\n",
        "            words.add(word)\n",
        "            if len(words) == size:\n",
        "                break\n",
        "        self.num_to_word = list(words)    \n",
        "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
        "\n",
        "    def numberize(self, word):\n",
        "        if word in self.word_to_num:\n",
        "            return self.word_to_num[word]\n",
        "        else: \n",
        "            return self.word_to_num['<UNK>']\n",
        "\n",
        "    def denumberize(self, num):\n",
        "        return self.num_to_word(num)\n",
        "\n",
        "chars = collections.Counter()\n",
        "for line in traindata:\n",
        "    chars.update(line)\n",
        "vocab = Vocab(chars, 100) # For our data, 100 is a good size."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-CBxHYz1pKO"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "Now we want to define a unigram language model. The parameters of the model are _logits_ $\\mathbf{s}$, which are unconstrained real numbers, and we will apply a softmax to change them into probabilities (which are nonnegative and sum to one).\n",
        "\n",
        "\\begin{align}\n",
        "P(i) &= [\\operatorname{softmax} \\mathbf{s}]_i \\\\\n",
        "&= \\frac{\\exp s_i}{\\sum_{i'} \\exp s_{i'}}.\n",
        "\\end{align}\n",
        "\n",
        "Create an array (a `Tensor`) of logits, one for each word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjxfIW1QR5T1"
      },
      "source": [
        "logits = torch.normal(mean=0, std=0.01, \n",
        "                      size=(vocab.size,), \n",
        "                      requires_grad=True, \n",
        "                      device=device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSrCsbknSKCu"
      },
      "source": [
        "The function `torch.normal` creates an array of random numbers, normally distributed (here with mean zero and standard deviation 0.01).\n",
        "\n",
        "The `size` argument says that it should be a one-dimensional array with `vocab.size` elements, one for each word in the vocabulary.\n",
        "\n",
        "The next two arguments are important. The `requires_grad` argument tells PyTorch that we will want to compute gradients with respect to `logits`, because we want to learn its values. The `device` argument says where to store the array.\n",
        "\n",
        "It will be useful to keep a list of all the parameters of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EWMvMbyTuvu"
      },
      "source": [
        "parameters = [logits]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw3lcnd3YVqp"
      },
      "source": [
        "Next, we write code to convert the logits into probabilities -- actually, log-probabilities. Torch has a function that does a softmax and a log together; it's more numerically stable than doing them in two steps. (Even though `logits` has only one dimension, we still have to say `dim=0` to specify which dimension the softmax should be computed over.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HTxqafBXC-O"
      },
      "source": [
        "def logprobs():\n",
        "    return torch.log_softmax(logits, dim=0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1JbNrXbhmEm"
      },
      "source": [
        "This returns an array of floats like you'd expect, but it also remembers _how_ it was computed. PyTorch will use this information to compute gradients for learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM0abXf41r9q"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Next, we create an optimizer, whose job is to adjust a set of parameters to minimize a loss function. Here, we're using `SGD` (stochastic gradient descent); other options are `Adagrad`, `Adam`, and others. Different optimizers take different options. Here, `lr` stands for \"learning rate\" and we usually try different powers of ten until we get the best results on the dev set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOg9hxbYJepL"
      },
      "source": [
        "o = torch.optim.SGD(parameters, lr=0.1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8MPOo1OJpUv"
      },
      "source": [
        "Next, we run through the training data a few times (epochs). For each sentence, move the parameters a little bit to decrease the loss function. If you want to rerun the training, go to **Run &rarr; Restart and run all** or **Runtime &rarr; Run all**. It takes about 5 minutes per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w050ph5ivows",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84423eb1-290e-4b28-8c14-b4980d2805e1"
      },
      "source": [
        "prev_dev_acc = None\n",
        "\n",
        "for epoch in range(100):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Run through the training data\n",
        "\n",
        "    random.shuffle(traindata) # Important\n",
        "\n",
        "    train_loss = 0\n",
        "    train_chars = 0\n",
        "    for chars in traindata:\n",
        "        nums = [vocab.numberize(char) for char in chars]\n",
        "\n",
        "        # Compute the negative log-likelihood of this line,\n",
        "        # which is the thing we want to minimize.\n",
        "        loss = 0.\n",
        "        for i in nums:\n",
        "            train_chars += 1\n",
        "            loss -= logprobs()[i]\n",
        "\n",
        "        # Keep a running total of negative log-likelihood.\n",
        "        # The .item() turns a one-element tensor into an ordinary float,\n",
        "        # including detaching the history of how it was computed,\n",
        "        # so we don't save the history across sentences.\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Compute gradient of loss with respect to parameters.\n",
        "        o.zero_grad()   # important: this must come first\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients (not needed here, but helpful for RNNs)\n",
        "        torch.nn.utils.clip_grad_norm_(parameters, 1.0)\n",
        "\n",
        "        # Do one step of gradient descent.\n",
        "        o.step()\n",
        "\n",
        "    # Run through the development data\n",
        "\n",
        "    dev_chars = dev_correct = 0\n",
        "    for chars in devdata:\n",
        "        nums = [vocab.numberize(char) for char in chars]\n",
        "        for i in nums:\n",
        "            dev_chars += 1\n",
        "\n",
        "            # Find the character with highest predicted probability.\n",
        "            # The .item() is needed for comparing with i.\n",
        "            best = logprobs().argmax().item()\n",
        "            if best == i:\n",
        "                dev_correct += 1\n",
        "\n",
        "    dev_acc = dev_correct/dev_chars\n",
        "    print(f'time={time.time()-epoch_start} train_ppl={math.exp(train_loss/train_chars)} dev_acc={dev_acc}')\n",
        "\n",
        "    # If dev accuracy got worse, halve the learning rate\n",
        "    if prev_dev_acc is not None and dev_acc <= prev_dev_acc:\n",
        "            o.param_groups[0]['lr'] *= 0.5\n",
        "            print(f\"lr={o.param_groups[0]['lr']}\")\n",
        "\n",
        "    # When the learning rate gets too low, stop training\n",
        "    if o.param_groups[0]['lr'] < 0.01:\n",
        "        break\n",
        "\n",
        "    prev_dev_acc = dev_acc"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time=176.4177610874176 train_ppl=27.169235823154356 dev_acc=0.16477499004380725\n",
            "time=173.23859167099 train_ppl=27.17237523692376 dev_acc=0.16477499004380725\n",
            "lr=0.05\n",
            "time=173.773264169693 train_ppl=27.117303524319112 dev_acc=0.16477499004380725\n",
            "lr=0.025\n",
            "time=172.58394718170166 train_ppl=27.090992142791837 dev_acc=0.16477499004380725\n",
            "lr=0.0125\n",
            "time=176.56773948669434 train_ppl=27.079781865860916 dev_acc=0.16477499004380725\n",
            "lr=0.00625\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}