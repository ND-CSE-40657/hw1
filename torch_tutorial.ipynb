{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Tutorial",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik9M2igT0XWf"
      },
      "source": [
        "import collections, time, math, random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWus4HvyKtos"
      },
      "source": [
        "# Writing a simple model in PyTorch\n",
        "\n",
        "This notebook shows you how to get started with PyTorch and also provides you some skeleton code. You can make a copy of the notebook and write your solution in it, or you can download it (**File &rarr; Download .py**) and work on it locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S13j4P_ZLjK5"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Clone the HW1 repository. (If you rerun the notebook, you'll get an error that directory `hw1` already exists, which you can ignore.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICSyqjzMLk2C",
        "outputId": "1297f92e-cf92-4fee-ec39-dda4b1e1b447"
      },
      "source": [
        "!git clone https://github.com/ND-CSE-40657/hw1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw1'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 82 (delta 18), reused 35 (delta 9), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (82/82), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-6Ff7j9Qu_v"
      },
      "source": [
        "Import PyTorch. If you want to run on your own computer, you'll need to install PyTorch, which is usually as simple as `pip install torch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1gqmSDrQ6Nc",
        "outputId": "467c1238-4926-43ab-fa52-f65dec967d7a"
      },
      "source": [
        "import torch\n",
        "print(f'Using Torch v{torch.__version__}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Torch v1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bNQMVSzTQu0"
      },
      "source": [
        "Check for a GPU. A GPU is not necessary for this assignment -- in fact, for the size of model we're training, it probably makes things slower. To enable/disable GPU, go to **Runtime &rarr; Change runtime type &rarr; Hardware accelerator** and select **GPU** (to enable the GPU) or **None** (to disable the GPU)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhOxkWlJQu5Q",
        "outputId": "164a2c2f-16d4-4492-dec4-4c7919b47c01"
      },
      "source": [
        "if torch.cuda.device_count() > 0:\n",
        "    print(f'Using GPU ({torch.cuda.get_device_name(0)})')\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print('Using CPU')\n",
        "    device = 'cpu'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s02IiCdSbno"
      },
      "source": [
        "## Read and preprocess data\n",
        "\n",
        "Read in the data files. Note that we strip trailing newlines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJipdajNwBZ-"
      },
      "source": [
        "def read_data(filename):\n",
        "    return [list(line.rstrip('\\n')) + ['<EOS>'] for line in open(filename)]\n",
        "traindata = read_data('hw1/data/train')\n",
        "devdata = read_data('hw1/data/dev')\n",
        "testdata = read_data('hw1/data/test')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiioHKwZxFqe"
      },
      "source": [
        "Create a vocabulary containing the most frequent words and some special words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmt1wc9zwTNN"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, counts, size):\n",
        "        if not isinstance(counts, collections.Counter):\n",
        "            raise TypeError('counts must be a collections.Counter')\n",
        "        words = {'<EOS>', '<UNK>'}\n",
        "        for word, _ in counts.most_common():\n",
        "            words.add(word)\n",
        "            if len(words) == size:\n",
        "                break\n",
        "        self.num_to_word = list(words)    \n",
        "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.num_to_word)\n",
        "    def __iter__(self):\n",
        "        return iter(self.num_to_word)\n",
        "\n",
        "    def numberize(self, word):\n",
        "        if word in self.word_to_num:\n",
        "            return self.word_to_num[word]\n",
        "        else: \n",
        "            return self.word_to_num['<UNK>']\n",
        "\n",
        "    def denumberize(self, num):\n",
        "        return self.num_to_word[num]\n",
        "\n",
        "chars = collections.Counter()\n",
        "for line in traindata:\n",
        "    chars.update(line)\n",
        "vocab = Vocab(chars, 100) # For our data, 100 is a good size."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-CBxHYz1pKO"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "Now we want to define a unigram language model. The parameters of the model are _logits_ $\\mathbf{s}$, which are unconstrained real numbers, and we will apply a softmax to change them into probabilities (which are nonnegative and sum to one).\n",
        "\n",
        "\\begin{align}\n",
        "P(i) &= [\\operatorname{softmax} \\mathbf{s}]_i \\\\\n",
        "&= \\frac{\\exp s_i}{\\sum_{i'} \\exp s_{i'}}.\n",
        "\\end{align}\n",
        "\n",
        "Create an array (a `Tensor`) of logits, one for each word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjxfIW1QR5T1"
      },
      "source": [
        "logits = torch.normal(mean=0, std=0.01, \n",
        "                      size=(len(vocab),), \n",
        "                      requires_grad=True, \n",
        "                      device=device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSrCsbknSKCu"
      },
      "source": [
        "The function `torch.normal` creates an array of random numbers, normally distributed (here with mean zero and standard deviation 0.01).\n",
        "\n",
        "The `size` argument says that it should be a one-dimensional array with `vocab.size` elements, one for each word in the vocabulary.\n",
        "\n",
        "The next two arguments are important. The `requires_grad` argument tells PyTorch that we will want to compute gradients with respect to `logits`, because we want to learn its values. The `device` argument says where to store the array.\n",
        "\n",
        "There are a couple of functions below that will want to know what the parameters of our model are. So we make a list for future use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EWMvMbyTuvu"
      },
      "source": [
        "parameters = [logits]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw3lcnd3YVqp"
      },
      "source": [
        "Next, we write code to convert the logits into probabilities -- actually, log-probabilities. Torch has a function that does a softmax and a log together; it's more numerically stable than doing them in two steps. (Even though `logits` has only one dimension, we still have to say `dim=0` to specify which dimension the softmax should be computed over.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HTxqafBXC-O"
      },
      "source": [
        "def logprobs():\n",
        "    return torch.log_softmax(logits, dim=0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1JbNrXbhmEm"
      },
      "source": [
        "This returns an array of floats like you'd expect, but this array also remembers _how_ it was computed. PyTorch will use this information to compute gradients for learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM0abXf41r9q"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Next, we create an optimizer, whose job is to adjust a set of parameters to minimize a loss function. Here, we're using `SGD` (stochastic gradient descent); other options are `Adagrad`, `Adam`, and others. Different optimizers take different options. Here, `lr` stands for \"learning rate\" and we usually try different powers of ten until we get the best results on the dev set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOg9hxbYJepL"
      },
      "source": [
        "o = torch.optim.SGD(parameters, lr=0.1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8MPOo1OJpUv"
      },
      "source": [
        "Next, we run through the training data a few times (epochs). For each sentence, move the parameters a little bit to decrease the loss function. If you want to rerun the training, go to **Run &rarr; Restart and run all** or **Runtime &rarr; Run all**. It takes about 5 minutes per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w050ph5ivows",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae11bc8-4d48-4fb6-822f-bb5c8596368c"
      },
      "source": [
        "prev_dev_acc = None\n",
        "\n",
        "for epoch in range(100):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Run through the training data\n",
        "\n",
        "    random.shuffle(traindata) # Important\n",
        "\n",
        "    train_loss = 0  # Total negative log-probability\n",
        "    train_chars = 0 # Total number of characters\n",
        "    for chars in traindata:\n",
        "        # Compute the negative log-likelihood of this line,\n",
        "        # which is the thing we want to minimize.\n",
        "        loss = 0.\n",
        "        for c in chars:\n",
        "            train_chars += 1\n",
        "            loss -= logprobs()[vocab.numberize(c)]\n",
        "\n",
        "        # Keep a running total of negative log-likelihood.\n",
        "        # The .item() turns a one-element tensor into an ordinary float,\n",
        "        # including detaching the history of how it was computed,\n",
        "        # so we don't save the history across sentences.\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Compute gradient of loss with respect to parameters.\n",
        "        o.zero_grad()   # Reset gradients to zero\n",
        "        loss.backward() # Add in the gradient of loss\n",
        "\n",
        "        # Clip gradients (not needed here, but helpful for RNNs)\n",
        "        torch.nn.utils.clip_grad_norm_(parameters, 1.0)\n",
        "\n",
        "        # Do one step of gradient descent.\n",
        "        o.step()\n",
        "\n",
        "    # Run through the development data\n",
        "\n",
        "    dev_chars = 0   # Total number of characters\n",
        "    dev_correct = 0 # Total number of characters guessed correctly\n",
        "    for chars in devdata:\n",
        "        for c in chars:\n",
        "            dev_chars += 1\n",
        "\n",
        "            # Find the character with highest predicted probability.\n",
        "            # The .item() is needed to change a one-element tensor to\n",
        "            # an ordinary int.\n",
        "            best = vocab.denumberize(logprobs().argmax().item())\n",
        "            if best == c:\n",
        "                dev_correct += 1\n",
        "\n",
        "    dev_acc = dev_correct/dev_chars\n",
        "    print(f'time={time.time()-epoch_start} train_ppl={math.exp(train_loss/train_chars)} dev_acc={dev_acc}')\n",
        "\n",
        "    # Important: If dev accuracy didn't improve, halve the learning rate\n",
        "    if prev_dev_acc is not None and dev_acc <= prev_dev_acc:\n",
        "            o.param_groups[0]['lr'] *= 0.5\n",
        "            print(f\"lr={o.param_groups[0]['lr']}\")\n",
        "\n",
        "    # When the learning rate gets too low, stop training\n",
        "    if o.param_groups[0]['lr'] < 0.01:\n",
        "        break\n",
        "\n",
        "    prev_dev_acc = dev_acc"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time=170.72065711021423 train_ppl=27.236687146749965 dev_acc=0.16477499004380725\n",
            "time=170.08737707138062 train_ppl=27.16790748501982 dev_acc=0.16477499004380725\n",
            "lr=0.05\n",
            "time=170.58567357063293 train_ppl=27.118215732586698 dev_acc=0.16477499004380725\n",
            "lr=0.025\n",
            "time=172.45511031150818 train_ppl=27.092211944102726 dev_acc=0.16477499004380725\n",
            "lr=0.0125\n",
            "time=169.34981751441956 train_ppl=27.07767936462796 dev_acc=0.16477499004380725\n",
            "lr=0.00625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsJCDl1wXcE1"
      },
      "source": [
        "## Matrix multiplication\n",
        "\n",
        "Not illustrated above is matrix multiplication using the [`@` operator](https://pytorch.org/docs/stable/generated/torch.matmul.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT09tYHoYVxL",
        "outputId": "a7c2d613-a959-4fae-d258-ec01e288ab87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "A = torch.ones(2,3)\n",
        "A"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO8MdizqYt06",
        "outputId": "88310103-6160-416d-8541-e76afcee0456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "b = torch.ones(3)\n",
        "b"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0dasSgiYwku",
        "outputId": "4d59b4d5-1350-42fc-d7b4-5d5918c828c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "C = torch.ones(3,5)\n",
        "C"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z9oP2T5YmH1"
      },
      "source": [
        "Matrix-vector product:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tkp9ONLYiXO",
        "outputId": "2f9561eb-50ea-4834-b655-5b22a27af4d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "A @ b"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCx8OmZFYoPD"
      },
      "source": [
        "Vector-matrix product:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5H2O3CFYpwL",
        "outputId": "2a05b8d0-8820-4f2f-b0d3-2d6ca8a143c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "b @ C"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 3., 3., 3., 3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAwFWeyFYx5J"
      },
      "source": [
        "Matrix-matrix product:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA-BPRD3YzL_",
        "outputId": "084bfd26-c8c7-4f57-fb09-0e4e34a827c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "A @ C"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3., 3., 3., 3., 3.],\n",
              "        [3., 3., 3., 3., 3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZgmnHc8ZpyV"
      },
      "source": [
        "The `@` operator works even if its arguments have more than two dimensions, but the semantics can be a little bit confusing. It's probably nicer to use [`einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html) instead. For example,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s70asFZgeSr",
        "outputId": "dbe8c5f3-e541-492f-d5cd-c5753e7f70c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "A = torch.ones(3,5,4)\n",
        "l = torch.ones(2,5)\n",
        "r = torch.ones(2,4)\n",
        "torch.einsum('bn,anm,bm->ba', l, A, r)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[20., 20., 20.],\n",
              "        [20., 20., 20.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDJkw3kKgrNR"
      },
      "source": [
        "This computes $B$ such that\n",
        "\n",
        "$$B_{ba} = \\sum_m \\sum_n l_{bn} A_{anm} r_{bm}$$\n",
        "\n",
        "where the summation is always over all index variables that appear to the left of the `->` but not to the right of the `->`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWTKoVTwad6s"
      },
      "source": [
        "## Saving and loading\n",
        "\n",
        "You may want to save a model to disk so you can continue training it later or use it later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdttQjHRdRXY"
      },
      "source": [
        "torch.save(parameters, 'model')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmdBXCvldVyX"
      },
      "source": [
        "parameters_copy = torch.load('model')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoMsIjyqdfWn"
      },
      "source": [
        "In Colab, howeer, the saved models won't persist across sessions. See the [Colab docs](https://colab.research.google.com/notebooks/io.ipynb) for some options for persistent storage."
      ]
    }
  ]
}